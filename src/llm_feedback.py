"""
ClƒÅrusAI: LLM Tutor Feedback (Ollama)

Generates bias-blind, 1‚Äì2 sentence reflective feedback after each reasoning stage.
Uses local LLaMA3 model via Ollama for reproducibility and offline generation.
"""

import subprocess
# from config import OLLAMA_CONFIG  # Optional: centralise model/temperature from config.py

def generate_stage_feedback(scenario, stage_index, user_response) -> str:
    """Generate short, bias-free tutor-style feedback using LLaMA3 via Ollama."""

    # Stage labels to specify task context
    stage_names = [
        "Primary Analysis", "Cognitive Factors",
        "Mitigation Strategies", "Transfer Learning"
    ]

    # Prompt fields mapped from scenario CSV
    prompt_fields = [
        "primary_prompt", "follow_up_1", "follow_up_2", "follow_up_3"
    ]

    # Construct prompt to pass to LLaMA3 model via Ollama
    prompt_text = (
        "You are an expert tutor giving reflective feedback to a trainee.\n"
        "The trainee has completed a professional reasoning task.\n\n"
        "You will see:\n"
        "‚Ä¢ The real-world scenario\n"
        "‚Ä¢ The task for this stage\n"
        "‚Ä¢ The user's written response\n\n"
        "Write a 1‚Äì2 sentence feedback:\n"
        "‚Ä¢ Start with one positive observation\n"
        "‚Ä¢ Then give one area for improvement\n"
        "‚Ä¢ Do NOT mention cognitive biases by name\n\n"
        f"--- SCENARIO ---\n{scenario.get('scenario_text', 'N/A')}\n\n"
        f"--- TASK ({stage_names[stage_index]}) ---\n{scenario.get(prompt_fields[stage_index], 'Prompt N/A')}\n\n"
        f"--- USER RESPONSE ---\n{user_response}\n\n"
        "üéì Feedback:"
    )

    try:
        # Call local LLaMA3 model via Ollama subprocess
        result = subprocess.run(
            ["ollama", "run", "llama3"],  # Optionally replace with: OLLAMA_CONFIG['model']
            input=prompt_text.encode(),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )

        # Decode model output
        feedback = result.stdout.decode().strip()

        # Fallback if model returns nothing
        if not feedback:
            return "‚ö†Ô∏è Feedback was empty. No response generated by LLaMA3."

        return feedback

    except Exception as e:
        # Fallback in case Ollama fails
        return "‚ö†Ô∏è Feedback could not be generated using Ollama."